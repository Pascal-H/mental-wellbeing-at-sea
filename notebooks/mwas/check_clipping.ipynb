{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "from main import load_database\n",
    "\n",
    "import audeer\n",
    "import audiofile as af\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before filtering out too few sessions: 27; Shape of the DataFrame before filtering: (3346, 152)\n",
      "Number of participants after filtering out too few sessions: 25; Shape of the DataFrame after filtering: (3304, 152)\n",
      "Shape of the DataFrame after filtering prompts: (3204, 152)\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "# Load the experiment configuration from the YAML file\n",
    "with open(\"../src/experiment_parameters.yaml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "df_files = load_database(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_clipping(samples, clipping_threshold, min_duration_millis, sample_rate):\n",
    "    def convert_time_ms_to_samples(time_ms, rate):\n",
    "        return int((time_ms / 1000.0) * rate)\n",
    "\n",
    "    min_duration_samples = convert_time_ms_to_samples(min_duration_millis, sample_rate)\n",
    "\n",
    "    is_clipping = False\n",
    "    num_clipped_samples = 0\n",
    "    for sample in samples:\n",
    "        cur_val = abs(sample)\n",
    "        if cur_val >= clipping_threshold:\n",
    "            num_clipped_samples += 1\n",
    "            if num_clipped_samples >= min_duration_samples:\n",
    "                is_clipping = True\n",
    "                break\n",
    "        else:\n",
    "            num_clipped_samples = 0\n",
    "\n",
    "    return is_clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "# Define the sampling rate\n",
    "sample_rate = 16000\n",
    "\n",
    "# Define the parameters for detect_clipping\n",
    "clipping_threshold = 0.99\n",
    "min_duration_millis = 0.5\n",
    "\n",
    "# Get the unique files\n",
    "unique_files = df_files.index.get_level_values(\"file\").unique()\n",
    "\n",
    "# Initialize a dictionary to store the clipping results\n",
    "clipping_results = {}\n",
    "\n",
    "# Loop over the unique files\n",
    "for file in audeer.progress_bar(unique_files, desc=\"Processing prompts\"):\n",
    "    # Read the audio file\n",
    "    samples, file_sample_rate = af.read(file)\n",
    "\n",
    "    # Check if the file's sample rate matches the expected sample rate\n",
    "    if file_sample_rate != sample_rate:\n",
    "        print(\n",
    "            f\"Warning: Sample rate of file {file} is {file_sample_rate}, but expected {sample_rate}.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Detect clipping\n",
    "    is_clipping = detect_clipping(\n",
    "        samples, clipping_threshold, min_duration_millis, sample_rate\n",
    "    )\n",
    "\n",
    "    # Store the result\n",
    "    clipping_results[file] = is_clipping\n",
    "\n",
    "# Add a column \"clipping\" to df_files\n",
    "df_files[\"clipping\"] = df_files.index.get_level_values(\"file\").map(clipping_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clipping\n",
       "False    3187\n",
       "True       17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files[\"clipping\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/13547a-32ceb67/384/080318c8-6933-4b48-a018-aba97374ab01.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/13547a-32ceb67/384/44829b1b-36ff-4e6e-9705-caa179ed4fe5.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/13547a-32ceb67/384/44b68a4b-e9cb-4811-93e1-3a200bbb5a41.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/13547a-32ceb67/384/5341738b-9901-4b5b-babe-8be602808992.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/13547a-32ceb67/384/ad5e1c7a-aea3-4230-8dbf-eddca615aaf7.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/319946-16c641e/300/25fdead0-8e42-48fe-8adc-8b785a36b3a9.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/319946-16c641e/301/76eae757-bf4f-488d-adc1-81a0d0109ea8.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/319946-16c641e/375/4547f3ed-0bfd-423e-b44a-5de9e77f05f9.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/319946-16c641e/385/b02d88d5-5239-4f39-b809-74de811037f4.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/319946-16c641e/502/eea1a3a3-6695-454f-a59e-85ec8d0214fa.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/275/cb174d13-126f-4c8e-a175-a491ab3e030a.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/275/d5389e52-2901-4a33-b91f-48f42ef60d4d.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/289/8303523b-5cca-45bd-b44f-74adaa2b8f15.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/336/e3406142-3375-488e-ac46-b4b6c3cb2466.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/446/eddec601-159c-4c2a-a522-fad86b06d0b6.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/506/0fd0be7e-457c-4d4c-aa9c-2443af21a03e.wav', '/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/raw/pilot-mental_wellbeing_at_sea/data/86e4dd-c0dd471/506/aa15dcf8-7f67-46f5-8b59-4adc5b1ba574.wav']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_files[df_files[\"clipping\"] == True].index.get_level_values(\"file\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(df_files[df_files[\"clipping\"] == True].index.get_level_values(\"file\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping in denoised files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before filtering out too few sessions: 27; Shape of the DataFrame before filtering: (3346, 152)\n",
      "Number of participants after filtering out too few sessions: 25; Shape of the DataFrame after filtering: (3304, 152)\n",
      "Shape of the DataFrame after filtering prompts: (3204, 152)\n"
     ]
    }
   ],
   "source": [
    "# Load the denoised data with dithering\n",
    "config[\"database\"][\n",
    "    \"path_data\"\n",
    "] = \"/data/share/aisoundlab-mental_wellbeing_at_sea/data_mwas_processed-final_data/facebook_denoiser-master64-converted_int16_dithering/pilot-mental_wellbeing_at_sea\"\n",
    "df_files_facebook_converted_dither = load_database(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "# Define the sampling rate\n",
    "sample_rate = 16000\n",
    "\n",
    "# Define the parameters for detect_clipping\n",
    "clipping_threshold = 0.99\n",
    "min_duration_millis = 0.5\n",
    "\n",
    "# Get the unique files\n",
    "unique_files_denoised = df_files_facebook_converted_dither.index.get_level_values(\n",
    "    \"file\"\n",
    ").unique()\n",
    "\n",
    "# Initialize a dictionary to store the clipping results\n",
    "clipping_results_denoised = {}\n",
    "\n",
    "# Loop over the unique files\n",
    "for file in audeer.progress_bar(unique_files_denoised, desc=\"Processing prompts\"):\n",
    "    # Read the audio file\n",
    "    samples, file_sample_rate = af.read(file)\n",
    "\n",
    "    # Check if the file's sample rate matches the expected sample rate\n",
    "    if file_sample_rate != sample_rate:\n",
    "        print(\n",
    "            f\"Warning: Sample rate of file {file} is {file_sample_rate}, but expected {sample_rate}.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Detect clipping\n",
    "    is_clipping = detect_clipping(\n",
    "        samples, clipping_threshold, min_duration_millis, sample_rate\n",
    "    )\n",
    "\n",
    "    # Store the result\n",
    "    clipping_results_denoised[file] = is_clipping\n",
    "\n",
    "# Add a column \"clipping\" to df_files_facebook_converted_dither\n",
    "df_files_facebook_converted_dither[\"clipping\"] = (\n",
    "    df_files_facebook_converted_dither.index.get_level_values(\"file\").map(\n",
    "        clipping_results_denoised\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clipping\n",
       "False    3204\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files_facebook_converted_dither[\"clipping\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    list(\n",
    "        df_files_facebook_converted_dither[\n",
    "            df_files_facebook_converted_dither[\"clipping\"] == True\n",
    "        ].index.get_level_values(\"file\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    list(\n",
    "        df_files_facebook_converted_dither[\n",
    "            df_files_facebook_converted_dither[\"clipping\"] == True\n",
    "        ].index.get_level_values(\"file\")\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_performance-static_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
